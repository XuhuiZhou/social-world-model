# VERL SFT Configuration for Social World Model
# This config replaces Together AI fine-tuning with local VERL training

# Data Configuration
data_dir: "data/tomi_results/socialized_context_o3-2025-04-16_rephrased_tomi_train_6000.csv_o3-2025-04-16"
output_dir: "training/verl_output"
val_ratio: 0.1
seed: 42

# Model Configuration
base_model: "microsoft/Phi-3-mini-4k-instruct"  # 3.8B model - good starting point
# Alternative models:
# - "microsoft/Phi-4-mini-instruct"           # 14B model - better quality, needs more VRAM
# - "meta-llama/Llama-3.2-3B-Instruct"        # 3B model - fast training
# - "Qwen/Qwen2.5-7B-Instruct"                # 7B model - good balance

model_max_length: 4096  # Increase if your contexts are longer

# Hardware Configuration
n_gpus: 3  # You have 3x RTX A6000

# Training Hyperparameters
micro_batch_size: 2            # Per GPU batch size (decrease if OOM)
gradient_accumulation_steps: 4  # Effective batch = 2 * 3 * 4 = 24
learning_rate: 1.0e-5
total_training_steps: 1000     # Total optimization steps
warmup_steps: 100              # LR warmup
save_freq: 100                 # Save checkpoint every N steps
val_freq: 50                   # Validate every N steps

# Parallelism (Advanced)
use_fsdp: true                 # Fully Sharded Data Parallel
use_sequence_parallel: false   # Only enable for very long sequences
sequence_parallel_size: 1

# Logging
wandb_project: "social-world-model-verl-sft"
wandb_run_name: null  # Auto-generated if null

# Notes:
# - Total training examples = train_size
# - Steps per epoch ≈ train_size / (micro_batch_size * n_gpus * gradient_accumulation_steps)
# - For 6000 examples: ~250 steps/epoch
# - 1000 steps ≈ 4 epochs
#
# Memory estimates (Phi-3-mini-4k-instruct, 3.8B params):
# - Per GPU: ~16-20GB with micro_batch_size=2
# - Safe for 3x RTX A6000 (48GB each)
#
# If you get OOM errors:
# 1. Reduce micro_batch_size to 1
# 2. Increase gradient_accumulation_steps to maintain effective batch size
# 3. Reduce model_max_length to 2048
